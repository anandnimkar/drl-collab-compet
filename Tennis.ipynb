{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, we implement a Multi-Agent Reinforcement Learning solution to the Unity ML-Agents Tennis environment for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sys import platform\n",
    "import os.path\n",
    "import time\n",
    "import random\n",
    "from itertools import count\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from typing import List\n",
    "from unityagents import UnityEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: nvidia-smi: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameters to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Tennis.app\"`\n",
    "- **Windows** (x86): `\"path/to/Tennis_Windows_x86/Tennis.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Tennis_Windows_x86_64/Tennis.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Tennis_Linux/Tennis.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Tennis_Linux/Tennis.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Tennis.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "```\n",
    "The code below is specific to the servers that were run to train this notebook. Please feel free to edit to ensure it works for your local environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "if platform.startswith('darwin'):\n",
    "    env = UnityEnvironment(file_name=\"./env/Tennis.app\")\n",
    "elif platform.startswith('linux'):\n",
    "    env = UnityEnvironment(file_name=\"./env/Tennis_Linux_NoVis/Tennis.x86_64\", no_graphics=True)\n",
    "else:\n",
    "    raise NotImplemented()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, we demonstrate how to use the Python API to control the agents and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you can watch the agents' performance, if they select actions at random with each time step. If you are using a non-headless environment, a window should pop up that allows you to observe the agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score (max over agents) from episode 1: 0.0\n",
      "Score (max over agents) from episode 2: 0.09000000171363354\n",
      "Score (max over agents) from episode 3: 0.0\n",
      "Score (max over agents) from episode 4: 0.0\n",
      "Score (max over agents) from episode 5: 0.09000000171363354\n"
     ]
    }
   ],
   "source": [
    "# for i in range(1, 6):                                      # play game for 5 episodes\n",
    "#     env_info = env.reset(train_mode=False)[brain_name]     # reset the environment\n",
    "#     states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "#     scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "#     while True:\n",
    "#         actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "#         actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "#         env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "#         next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "#         rewards = env_info.rewards                         # get reward (for each agent)\n",
    "#         dones = env_info.local_done                        # see if episode finished\n",
    "#         scores += env_info.rewards                         # update the score (for each agent)\n",
    "#         states = next_states                               # roll over states to next time step\n",
    "#         if np.any(dones):                                  # exit loop if episode finished\n",
    "#             break\n",
    "#     print('Score (max over agents) from episode {}: {}'.format(i, np.max(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. Implementation\n",
    "\n",
    "Now we will train our own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MultiAgentReplayBuffer(object):\n",
    "    def __init__(self, max_buffer_size, num_agents, state_shape, action_size):\n",
    "        \"\"\"\n",
    "        Initialize a ReplayBuffer object.\n",
    "        :param max_buffer_size: maximum size of the buffer\n",
    "        :param num_agents: number of agents\n",
    "        :param state_shape: shape of state with first dimension being the number of agents\n",
    "        :param action_size: size of each action\n",
    "        \"\"\"\n",
    "        self.max_buffer_size = max_buffer_size\n",
    "        self.num_agents = num_agents\n",
    "        self.state_shape = state_shape\n",
    "        self.action_size = action_size\n",
    "        self.states = np.full((self.max_buffer_size, *self.state_shape), np.nan)\n",
    "        self.actions = np.full((self.max_buffer_size, self.num_agents, self.action_size), np.nan)\n",
    "        self.rewards = np.full((self.max_buffer_size, self.num_agents, 1), np.nan)\n",
    "        self.next_states = np.full((self.max_buffer_size, *self.state_shape), np.nan)\n",
    "        self.dones = np.full((self.max_buffer_size, self.num_agents, 1), np.nan)\n",
    "        self.buffer_size = 0\n",
    "        self.idx = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the current size of the buffer.\n",
    "        :return: current size of the buffer\n",
    "        \"\"\"\n",
    "        return self.buffer_size\n",
    "\n",
    "    def upsert(self, states, actions, rewards, next_states, dones):\n",
    "        \"\"\"\n",
    "        Insert samples into the buffer and increment the index.\n",
    "        :param states: np.array of shape (num_agents, state_size)\n",
    "        :param actions: np.array of shape (num_agents, action_size)\n",
    "        :param rewards: np.array of shape (num_agents, 1)\n",
    "        :param next_states: np.array of shape (num_agents, state_size)\n",
    "        :param dones: np.array of shape (num_agents, 1)\n",
    "        \"\"\"\n",
    "        self.states[self.idx] = states\n",
    "        self.actions[self.idx] = actions\n",
    "        self.rewards[self.idx] = rewards\n",
    "        self.next_states[self.idx] = next_states\n",
    "        self.dones[self.idx] = dones\n",
    "        self.buffer_size = min(self.buffer_size + 1, self.max_buffer_size)\n",
    "        self.idx = (self.idx + 1) % self.max_buffer_size\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        if batch_size > self.buffer_size:\n",
    "            raise ValueError(\"Not enough samples in buffer\")\n",
    "        idx = np.random.randint(0, self.idx, batch_size)\n",
    "        return {\n",
    "            'states': self.states[idx],\n",
    "            'states_flat': self.states[idx].reshape(batch_size, self.states[idx].size),\n",
    "            'actions': self.actions[idx],\n",
    "            'actions_flat': self.actions[idx].reshape(batch_size, self.actions[idx].size),\n",
    "            'rewards': self.rewards[idx],\n",
    "            'next_states': self.next_states[idx],\n",
    "            'next_states_flat': self.next_states[idx].reshape(batch_size, self.actions[idx].size),\n",
    "            'dones': self.dones[idx]\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MATD3TwinCriticNetwork(nn.Module):\n",
    "    def __init__(self, all_agents_state_size: int, all_agents_action_size: int, hidden_dims: List[int]=(256, 256), activation_fn = F.relu, device=torch.device('cpu')):\n",
    "        \"\"\"\n",
    "        Centralized action-value function approximator that takes as input the actions of all agents and the states of all agents, and outputs the Q-value for each agent.\n",
    "        :param all_agents_state_size: Dimension of the states of all agents concatenated.\n",
    "        :param all_agents_action_size: Dimension of the actions of all agents concatenated.\n",
    "        :param hidden_dims: List of hidden dimensions for the fully connected layers.\n",
    "        :param activation_fn: Activation function for the fully connected layers.\n",
    "        :param device: Device on which to run the neural network.\n",
    "        \"\"\"\n",
    "        super(MATD3TwinCriticNetwork, self).__init__()\n",
    "        self.device = device\n",
    "        self.all_agents_state_size = all_agents_state_size\n",
    "        self.all_agents_action_size = all_agents_action_size\n",
    "        self.activation_fn = activation_fn\n",
    "\n",
    "        input_dim = self.all_agents_state_size + self.all_agents_action_size\n",
    "        output_dim = 1\n",
    "\n",
    "        self.layers_a = nn.ModuleList()\n",
    "        self.layers_b = nn.ModuleList()\n",
    "        for i, dim in enumerate(hidden_dims):\n",
    "            if i == 0:\n",
    "                self.layers_a.append(nn.Linear(input_dim, dim))\n",
    "                self.layers_b.append(nn.Linear(input_dim, dim))\n",
    "            else:\n",
    "                self.layers_a.append(nn.Linear(hidden_dims[i-1], dim))\n",
    "                self.layers_b.append(nn.Linear(hidden_dims[i-1], dim))\n",
    "            self.layers_a.append(activation_fn)\n",
    "            self.layers_b.append(activation_fn)\n",
    "        self.layers_a.append(nn.Linear(hidden_dims[-1], output_dim))\n",
    "        self.layers_b.append(nn.Linear(hidden_dims[-1], output_dim))\n",
    "        self.layers_a.append(nn.Tanh())\n",
    "        self.layers_b.append(nn.Tanh())\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, states, actions):\n",
    "        \"\"\"\n",
    "        Takes as input the states and actions of all agents, and outputs the Q-value for each agent.\n",
    "        :param states: States of all agents concatenated.\n",
    "        :param actions: Actions of all agents concatenated.\n",
    "        \"\"\"\n",
    "        s, a = self._format(states, actions)\n",
    "        x = torch.cat([s, a], dim=1)\n",
    "        for layer_a, layer_b in zip(self.layers_a, self.layers_b):\n",
    "            xa = layer_a(x)\n",
    "            xb = layer_b(x)\n",
    "        return xa, xb\n",
    "\n",
    "    def q_a(self, states, actions):\n",
    "        \"\"\"\n",
    "        Takes as input the states and actions of all agents, and outputs the Qa-value for each agent.\n",
    "        :param states: States of all agents concatenated.\n",
    "        :param actions: Actions of all agents concatenated.\n",
    "        \"\"\"\n",
    "        s, a = self._format(states, actions)\n",
    "        x = torch.cat([s, a], dim=1)\n",
    "        for layer_a in self.layers_a:\n",
    "            xa = layer_a(x)\n",
    "        return xa\n",
    "\n",
    "    def _format(self, states, actions):\n",
    "        s, a = states, actions\n",
    "        if not isinstance(s, torch.Tensor):\n",
    "            s = torch.tensor(s,\n",
    "                             device=self.device,\n",
    "                             dtype=torch.float32)\n",
    "        if not isinstance(a, torch.Tensor):\n",
    "            a = torch.tensor(a,\n",
    "                             device=self.device,\n",
    "                             dtype=torch.float32)\n",
    "        if s.dim() == 1:\n",
    "            s = s.unsqueeze(0)\n",
    "        if a.dim() == 1:\n",
    "            a = a.unsqueeze(0)\n",
    "        assert s.dim() == 2 and a.dim() == 2\n",
    "        return s, a"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MATD3ActorNetwork(nn.Module):\n",
    "    def __init__(self, state_size: int, action_size: int, hidden_dims=(256,256), activation_fn = F.relu, device=torch.device(self.config['device'])):\n",
    "        \"\"\"\n",
    "        Policy that takes as input the state of an agent and outputs the actions for that agent.\n",
    "        :param state_size: Dimension of the state of a single agent.\n",
    "        :param action_size: Dimension of the actions of a single agent.\n",
    "        :param hidden_dims: List of hidden dimensions for the fully connected layers.\n",
    "        :param activation_fn: Activation function for the fully connected layers.\n",
    "        :param device: Device on which to run the neural network.\n",
    "        \"\"\"\n",
    "        super(MATD3ActorNetwork, self).__init__()\n",
    "        self.device = device\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.activation_fn = activation_fn\n",
    "\n",
    "        input_dim = state_size\n",
    "        output_dim = action_size\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i, dim in enumerate(hidden_dims):\n",
    "            if i == 0:\n",
    "                self.layers.append(nn.Linear(input_dim, dim))\n",
    "            else:\n",
    "                self.layers.append(nn.Linear(hidden_dims[i-1], dim))\n",
    "            self.layers.append(activation_fn)\n",
    "        self.layers.append(nn.Linear(hidden_dims[-1], output_dim))\n",
    "        self.layers.append(nn.Tanh())\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self._format(state)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def _format(self, state):\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            state = torch.tensor(state,\n",
    "                                 device=self.device,\n",
    "                                 dtype=torch.float32)\n",
    "        if state.dim() == 1:\n",
    "            state = state.unsqueeze(0)\n",
    "        assert state.dim() == 2\n",
    "        return state"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "class MATD3Agent:\n",
    "    def __init__(self, state_size, action_size, all_agents_state_size, all_agents_action_size, config):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.all_agents_state_size = all_agents_state_size\n",
    "        self.all_agents_action_size = all_agents_action_size\n",
    "        self.config = config\n",
    "\n",
    "        self.critic_online = MATD3TwinCriticNetwork(self.all_agents_state_size, self.all_agents_action_size, hidden_dims=self.config['critic_hidden_dims'], activation_fn=self.config['critic_activation_fn'], device=torch.device(self.config['device']))\n",
    "        self.critic_target = MATD3TwinCriticNetwork(self.all_agents_state_size, self.all_agents_action_size, hidden_dims=self.config['critic_hidden_dims'], activation_fn=self.config['critic_activation_fn'], device=torch.device(self.config['device']))\n",
    "        self.update_critic_target(1.0)\n",
    "\n",
    "        self.actor_online = MATD3ActorNetwork(self.state_size, self.action_size, hidden_dims=self.config['actor_hidden_dims'], activation_fn=self.config['actor_activation_fn'], device=torch.device(self.config['device']))\n",
    "        self.actor_target = MATD3ActorNetwork(self.state_size, self.action_size, hidden_dims=self.config['actor_hidden_dims'], activation_fn=self.config['actor_activation_fn'], device=torch.device(self.config['device']))\n",
    "        self.update_actor_target(1.0)\n",
    "\n",
    "        self.critic_optimizer = optim.Adam(self.critic_online.parameters(), lr=self.config['critic_lr'])\n",
    "        self.actor_optimizer = optim.Adam(self.actor_online.parameters(), lr=self.config['actor_lr'])\n",
    "\n",
    "    def optimize(self, agent_idx: int, experiences, target_next_actions, optimize_policy: bool):\n",
    "        \"\"\"\n",
    "        Optimizes the models of the agent.\n",
    "        :param agent_idx: Index of this agent.\n",
    "        :param experiences: Experience sample from the MultiAgentReplayBuffer of all agents.\n",
    "        :param target_next_actions: Numpy array of shape (num_agents, batch_size, action_size) containing the actions of the next state for each agent from the target policy.\n",
    "        :param optimize_policy: Whether to optimize the policy or not.\n",
    "        \"\"\"\n",
    "\n",
    "        # online value model loads experiences into device\n",
    "\n",
    "        batch_size = len(experiences['dones'])\n",
    "        states = experiences['states']\n",
    "        states_flat = experiences['states_flat']\n",
    "        states_self = experiences['states'][:, agent_idx, :]\n",
    "        actions = experiences['actions']\n",
    "        actions_flat = experiences['actions_flat']\n",
    "        rewards_self = experiences['rewards'][:, agent_idx]\n",
    "        next_states_flat = experiences['next_states_flat']\n",
    "        dones_self = experiences['dones'][:, agent_idx]\n",
    "\n",
    "        target_next_actions_flats = target_next_actions.transpose(1,0,2).reshape(batch_size, -1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            target_qa, target_qb = self.critic_target(next_states_flat, target_next_actions_flats)\n",
    "            target_q = torch.from_numpy(rewards_self) + \\\n",
    "                       torch.from_numpy((1 - dones_self)) * \\\n",
    "                       self.config['gamma'] * \\\n",
    "                       torch.min(target_qa.to('cpu'), target_qb.to('cpu'))\n",
    "            target_q = target_q.to(self.critic_online.device)\n",
    "\n",
    "        qa, qb = self.critic_online(states_flat, actions_flat)\n",
    "        value_loss = F.mse_loss(qa, target_q) + F.mse_loss(qb, target_q)\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        nn.utils.clip_grad_norm(self.critic_online.parameters(), self.config['critic_gradient_clip_value'])\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        if optimize_policy:\n",
    "            greedy_action = self.actor_online(states_self)\n",
    "            actions_copy = np.copy(actions)\n",
    "            actions_copy[:, agent_idx] = greedy_action.detach().cpu().numpy()\n",
    "            q_value = self.critic_online.q_a(states, actions_copy.reshape(batch_size, -1))\n",
    "            policy_loss = -q_value.mean()\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.actor_online.parameters(), self.config['actor_gradient_clip_value'])\n",
    "            self.actor_optimizer.step()\n",
    "\n",
    "    def select_online_action(self, state, noise_std_dev: float, noise_clip_range: float):\n",
    "        greedy_action = self.actor_online(state).cpu().detach().numpy()\n",
    "        noise = np.random.normal(0.0, scale=noise_std_dev, size=self.action_size)\n",
    "        noise = np.clip(noise, -abs(noise_clip_range), abs(noise_clip_range))\n",
    "        noisy_action = np.clip(greedy_action + noise, -1, 1) # TODO dynamically rescale clip of action to support other environments; currently it's between -1 and 1 only\n",
    "        return noisy_action\n",
    "\n",
    "    def select_target_action(self, state, noise_std_dev: float, noise_clip_range: float, use_numpy: bool=True):\n",
    "        greedy_action = self.actor_target(state)\n",
    "        noise = torch.randn_like(greedy_action) * noise_std_dev\n",
    "        noise = torch.clamp(noise, -noise_clip_range, noise_clip_range)\n",
    "        noisy_action = greedy_action + noise\n",
    "        noisy_action = torch.clamp(noisy_action, -1, 1) # TODO dynamically rescale clip of action to support other environments; currently it's between -1 and 1 only\n",
    "        return noisy_action.cpu().detach().numpy() if use_numpy else noisy_action\n",
    "\n",
    "    def update_critic_target(self, tau: float):\n",
    "        self._update_target(self.critic_target, self.critic_online, tau)\n",
    "\n",
    "    def update_actor_target(self, tau: float):\n",
    "        self._update_target(self.actor_target, self.actor_online, tau)\n",
    "\n",
    "    def save_checkpoints(self, name: str):\n",
    "        print(\"Saving checkpoints for {}\".format(name))\n",
    "        torch.save({\n",
    "            'critic_online': self.critic_online.state_dict(),\n",
    "            'critic_optimizer': self.critic_optimizer.state_dict(),\n",
    "            'actor_online': self.actor_online.state_dict(),\n",
    "            'actor_optimizer': self.actor_optimizer.state_dict(),\n",
    "        }, os.path.join(self.config['checkpoint_dir'], '{}.tar'.format(name)))\n",
    "\n",
    "    def load_checkpoints(self, name: str, train_mode: bool=False):\n",
    "        message = \"Loading checkpoints for {}\".format(name)\n",
    "        if train_mode:\n",
    "            message += \" in train mode\"\n",
    "        else:\n",
    "            message += \" in eval mode\"\n",
    "        print(message)\n",
    "\n",
    "        checkpoint = torch.load(os.path.join(self.config['checkpoint_dir'], '{}.tar'.format(name)))\n",
    "        self.critic_online.load_state_dict(checkpoint['critic_online'])\n",
    "        self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer'])\n",
    "        self.actor_online.load_state_dict(checkpoint['actor_online'])\n",
    "        self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer'])\n",
    "\n",
    "        if train_mode:\n",
    "            self.critic_online.train()\n",
    "            self.actor_online.train()\n",
    "        else:\n",
    "            self.critic_online.eval()\n",
    "            self.actor_online.eval()\n",
    "\n",
    "    @staticmethod\n",
    "    def _update_target(target, online, tau: float):\n",
    "        for target_param, param in zip(target.parameters(), online.parameters()):\n",
    "            target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-6-4239542c8632>, line 81)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  File \u001B[0;32m\"<ipython-input-6-4239542c8632>\"\u001B[0;36m, line \u001B[0;32m81\u001B[0m\n\u001B[0;31m    message += \", train score 100ep {:.2f}.format(mean_100_reward)\u001B[0m\n\u001B[0m                                                                  ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "class MATD3Trainer():\n",
    "    def __init__(self, config, env):\n",
    "        self.config = config\n",
    "        torch.manual_seed(self.config['seed']); np.random.seed(self.config['seed']); random.seed(self.config['seed'])\n",
    "\n",
    "        self.env = env\n",
    "        self.brain_name = self.env.brain_names[0]\n",
    "        self.brain = self.env.brains[self.brain_name]\n",
    "        env_info = self.env.reset(train_mode=True)[self.brain_name]\n",
    "        self.num_agents = len(env_info.agents)\n",
    "\n",
    "        self.action_size_per_agent = self.brain.vector_action_space_size\n",
    "        self.all_agents_state_size = env_info.vector_observations.size\n",
    "        self.all_agents_action_size = self.action_size_per_agent * self.num_agents\n",
    "\n",
    "        self.replay_buffer = MultiAgentReplayBuffer(self.config['max_buffer_size'], self.num_agents, env_info.vector_observations.shape, self.action_size_per_agent)\n",
    "\n",
    "        self.agents = [MATD3Agent(env_info.vector_observations[agent_idx].size, action_size, self.all_agents_state_size, self.all_agents_action_size, self.config) for agent_idx in range(self.num_agents)]\n",
    "\n",
    "    def train(self):\n",
    "        self.episode_rewards = []\n",
    "        self.mean_100_episode_rewards = []\n",
    "        self.episode_timesteps = []\n",
    "        self.episode_timestamps = []\n",
    "        self.episode_durations = []\n",
    "\n",
    "        training_start = time.time()\n",
    "        print(\"Training...\")\n",
    "        for episode in range(1, self.config['max_episodes'] + 1):\n",
    "            episode_start = time.time()\n",
    "            env_info = self.env.reset(train_mode=True)[self.brain_name]\n",
    "            rewards_sum = np.zeros(self.num_agents)\n",
    "\n",
    "            for step in count(1):\n",
    "                states = env_info.vector_observations\n",
    "                actions = [agent.select_online_action(states[agent_idx], self.config['exploration_noise_std_dev'], self.config['exploration_noise_clip_range']) for agent_idx, agent in enumerate(self.agents)]\n",
    "                actions = np.array(actions)\n",
    "                env_info = self.env.step(actions)[self.brain_name]\n",
    "                next_states = env_info.vector_observations\n",
    "                rewards = env_info.rewards\n",
    "                dones = env_info.local_done\n",
    "\n",
    "                self.replay_buffer.upsert(states, actions, rewards, next_states, dones)\n",
    "                rewards_sum += rewards\n",
    "\n",
    "                if len(self.replay_buffer) >= self.config['min_buffer_size']:\n",
    "                    experiences = self.replay_buffer.sample(self.config['batch_size'])\n",
    "                    noisy_target_next_actions = [agent.select_target_action(experiences['next_states'][:, agent_idx, :], self.config['tps_noise_std_dev'], self.config['tps_noise_clip_range'], use_numpy=True) for agent_idx, agent in enumerate(self.agents)]\n",
    "                    optimize_policy = step % self.config['optimize_policy_every_steps'] == 0\n",
    "                    for agent_idx, agent in enumerate(self.agents):\n",
    "                        agent.optimize(agent_idx, experiences, noisy_target_next_actions, optimize_policy)\n",
    "\n",
    "                if step % self.config['update_critic_target_every_steps'] == 0:\n",
    "                    for agent in self.agents:\n",
    "                        agent.update_critic_target(self.config['tau'])\n",
    "\n",
    "                if step % self.config['update_actor_target_every_steps'] == 0:\n",
    "                    for agent in self.agents:\n",
    "                        agent.update_actor_target(self.config['tau'])\n",
    "\n",
    "                if np.any(dones):\n",
    "                    self.episode_timesteps.append(step)\n",
    "                    self.episode_rewards.append(rewards_sum)\n",
    "                    episode_end = time.time()\n",
    "                    self.episode_timestamps.append(episode_end)\n",
    "                    episode_duration = episode_end - episode_start\n",
    "                    self.episode_durations.append(episode_duration)\n",
    "                    break\n",
    "\n",
    "            mean_100_reward = np.max(self.episode_rewards[-100:], axis=1).mean() if len(self.episode_rewards) >= 100 else np.nan\n",
    "            goal_reached = mean_100_reward >= self.config['goal_mean_100_reward']\n",
    "\n",
    "            duration = time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - training_start))\n",
    "            message = (\"{} - episode {:05}, timesteps {:07}, train score 1ep {:.2f}\".format(\n",
    "                duration,\n",
    "                episode,\n",
    "                np.sum(self.episode_timesteps, dtype=np.intc),\n",
    "                np.max(self.episode_rewards[-1:], axis=1).mean(),\n",
    "                mean_100_reward))\n",
    "            message += \", train score 100ep {:.2f}\".format(mean_100_reward) if mean_100_reward is not np.nan else \"\"\n",
    "            print(message)\n",
    "\n",
    "            if goal_reached:\n",
    "                print(\"Goal reached! Training stopped.\")\n",
    "                self.save_checkpoints(episode)\n",
    "                break\n",
    "\n",
    "            if episode == self.config['max_episodes']:\n",
    "                print(\"Max episodes reached! Training stopped.\")\n",
    "                self.save_checkpoints(episode)\n",
    "                break\n",
    "\n",
    "            if episode % self.config['save_every_episodes'] == 0:\n",
    "                self.save_checkpoints(episode)\n",
    "\n",
    "    def save_checkpoints(self, episode: int):\n",
    "        for agent_idx, agent in enumerate(self.agents):\n",
    "            agent.save_checkpoints('agent{}_episode{:05}'.format(agent_idx, episode))\n",
    "\n",
    "    def load_checkpoints(self, episode: int, train_mode: bool = False):\n",
    "        for agent_idx, agent in enumerate(self.agents):\n",
    "            agent.load_checkpoints('agent{}_episode{:05}'.format(agent_idx, episode), train_mode)\n",
    "\n",
    "    def demo(self, checkpoint_episode: int):\n",
    "        self.load_checkpoints(checkpoint_episode, train_mode=False)\n",
    "        env_info = self.env.reset(train_mode=False)[self.brain_name]\n",
    "        rewards_sum = np.zeros(self.num_agents)\n",
    "        while True:\n",
    "            states = env_info.vector_observations\n",
    "            actions = [agent.select_online_action(states[agent_idx], 0) for agent_idx, agent in enumerate(self.agents)]\n",
    "            actions = np.array(actions)\n",
    "            env_info = self.env.step(actions)[self.brain_name]\n",
    "            rewards = env_info.rewards\n",
    "            rewards_sum += rewards\n",
    "            dones = env_info.local_done\n",
    "            if np.any(dones):\n",
    "                break\n",
    "        return rewards_sum, np.max(rewards_sum, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "config = {\n",
    "    'critic_hidden_dims': [256, 256],\n",
    "    'critic_activation_fn': F.relu,\n",
    "    'critic_lr': 3e-4,\n",
    "    'critic_gradient_clip_value': float('inf'),\n",
    "    'actor_hidden_dims': [256, 256],\n",
    "    'actor_activation_fn': F.relu,\n",
    "    'actor_lr': 3e-4,\n",
    "    'actor_gradient_clip_value': float('inf'),\n",
    "    'gamma': 0.99,\n",
    "    'tau': 0.01,\n",
    "    'checkpoint_dir': os.path.join(os.path.dirname(os.path.realpath('__file__')), 'checkpoints'),\n",
    "    'max_buffer_size': 1000000,\n",
    "    'min_buffer_size': 1000,\n",
    "    'batch_size': 250,\n",
    "    'max_episodes': 50000,\n",
    "    'exploration_noise_std_dev': 0.1,\n",
    "    'exploration_noise_clip_range': float('inf'),\n",
    "    'tps_noise_std_dev': 0.1,\n",
    "    'tps_noise_clip_range': 0.5,\n",
    "    'seed': 42,\n",
    "    'device': 'cuda:0', # TODO - enable DistributedDataParallel for multi-GPU training\n",
    "}\n",
    "matd3 = MATD3Trainer(config, env)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 5, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": "[[[-1, 1], [-2, 2], [-3, 3], [-4, 4], [-5, 5]],\n [[-1.1, 1.1], [-2.1, 2.1], [-3.1, 3.1], [-4.1, 4.1], [-5.1, 5.1]]]"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1 = []\n",
    "b2 = []\n",
    "for i in range(1, 6):\n",
    "    a1 = [-i, i]\n",
    "    a2 = [-i-.1, i+.1]\n",
    "    b1.append(a1)\n",
    "    b2.append(a2)\n",
    "c = [b1, b2]\n",
    "print(np.array(c).shape)\n",
    "c"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "c_t = torch.tensor(c)\n",
    "c_t2 = torch.transpose(c_t, 0, 1).reshape(5, -1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 5, 4])"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_t2.unsqueeze(0).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ True,  True,  True,  True],\n       [ True,  True,  True,  True],\n       [ True,  True,  True,  True],\n       [ True,  True,  True,  True],\n       [ True,  True,  True,  True]])"
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_t2_np = c_t2.cpu().detach().numpy()\n",
    "c_t_np = c_t.cpu().detach().numpy()\n",
    "c_t_np = c_t_np.transpose(1,0,2).reshape(5, -1)\n",
    "\n",
    "c_t_np == c_t2_np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}