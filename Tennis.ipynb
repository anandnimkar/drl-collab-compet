{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, we implement a Multi-Agent Reinforcement Learning solution to the Unity ML-Agents Tennis environment for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sys import platform\n",
    "import os.path\n",
    "import time\n",
    "import random\n",
    "from itertools import count\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from typing import List\n",
    "from unityagents import UnityEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameters to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Tennis.app\"`\n",
    "- **Windows** (x86): `\"path/to/Tennis_Windows_x86/Tennis.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Tennis_Windows_x86_64/Tennis.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Tennis_Linux/Tennis.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Tennis_Linux/Tennis.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Tennis.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "```\n",
    "The code below is specific to the servers that were run to train this notebook. Please feel free to edit to ensure it works for your local environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "if platform.startswith('darwin'):\n",
    "    env = UnityEnvironment(file_name=\"./env/Tennis.app\")\n",
    "elif platform.startswith('linux'):\n",
    "    env = UnityEnvironment(file_name=\"./env/Tennis_Linux_NoVis/Tennis.x86_64\", no_graphics=True)\n",
    "else:\n",
    "    raise NotImplemented()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "48"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states.size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, we demonstrate how to use the Python API to control the agents and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you can watch the agents' performance, if they select actions at random with each time step. If you are using a non-headless environment, a window should pop up that allows you to observe the agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score (max over agents) from episode 1: 0.0\n",
      "Score (max over agents) from episode 2: 0.09000000171363354\n",
      "Score (max over agents) from episode 3: 0.0\n",
      "Score (max over agents) from episode 4: 0.0\n",
      "Score (max over agents) from episode 5: 0.09000000171363354\n"
     ]
    }
   ],
   "source": [
    "# for i in range(1, 6):                                      # play game for 5 episodes\n",
    "#     env_info = env.reset(train_mode=False)[brain_name]     # reset the environment\n",
    "#     states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "#     scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "#     while True:\n",
    "#         actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "#         actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "#         env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "#         next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "#         rewards = env_info.rewards                         # get reward (for each agent)\n",
    "#         dones = env_info.local_done                        # see if episode finished\n",
    "#         scores += env_info.rewards                         # update the score (for each agent)\n",
    "#         states = next_states                               # roll over states to next time step\n",
    "#         if np.any(dones):                                  # exit loop if episode finished\n",
    "#             break\n",
    "#     print('Score (max over agents) from episode {}: {}'.format(i, np.max(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. Implementation\n",
    "\n",
    "Now we will train our own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MultiAgentReplayBuffer(object):\n",
    "    def __init__(self, max_buffer_size, num_agents, state_shape, action_size):\n",
    "        \"\"\"\n",
    "        Initialize a ReplayBuffer object.\n",
    "        :param max_buffer_size: maximum size of the buffer\n",
    "        :param num_agents: number of agents\n",
    "        :param state_shape: shape of state with first dimension being the number of agents\n",
    "        :param action_size: size of each action\n",
    "        \"\"\"\n",
    "        self.max_buffer_size = max_buffer_size\n",
    "        self.num_agents = num_agents\n",
    "        self.state_shape = state_shape\n",
    "        self.action_size = action_size\n",
    "        self.states = np.full((self.max_buffer_size, *self.state_shape), np.nan)\n",
    "        self.actions = np.full((self.max_buffer_size, self.num_agents, self.action_size), np.nan)\n",
    "        self.rewards = np.full((self.max_buffer_size, self.num_agents, 1), np.nan)\n",
    "        self.next_states = np.full((self.max_buffer_size, *self.state_shape), np.nan)\n",
    "        self.dones = np.full((self.max_buffer_size, self.num_agents, 1), np.nan)\n",
    "        self.buffer_size = 0\n",
    "        self.idx = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the current size of the buffer.\n",
    "        :return: current size of the buffer\n",
    "        \"\"\"\n",
    "        return self.buffer_size\n",
    "\n",
    "    def upsert(self, states, actions, rewards, next_states, dones):\n",
    "        \"\"\"\n",
    "        Insert samples into the buffer and increment the index.\n",
    "        :param states: np.array of shape (num_agents, state_size)\n",
    "        :param actions: np.array of shape (num_agents, action_size)\n",
    "        :param rewards: np.array of shape (num_agents, 1)\n",
    "        :param next_states: np.array of shape (num_agents, state_size)\n",
    "        :param dones: np.array of shape (num_agents, 1)\n",
    "        \"\"\"\n",
    "        self.states[self.idx] = states\n",
    "        self.actions[self.idx] = actions\n",
    "        self.rewards[self.idx] = rewards\n",
    "        self.next_states[self.idx] = next_states\n",
    "        self.dones[self.idx] = dones\n",
    "        self.buffer_size = min(self.buffer_size + 1, self.max_buffer_size)\n",
    "        self.idx = (self.idx + 1) % self.max_buffer_size\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        if batch_size > self.buffer_size:\n",
    "            raise ValueError(\"Not enough samples in buffer\")\n",
    "        idx = np.random.randint(0, self.idx, batch_size)\n",
    "        return {\n",
    "            'states': self.states[idx],\n",
    "            'states_flat': self.states[idx].reshape(batch_size, self.states[idx].size),\n",
    "            'actions': self.actions[idx],\n",
    "            'actions_flat': self.actions[idx].reshape(batch_size, self.actions[idx].size),\n",
    "            'rewards': self.rewards[idx],\n",
    "            'next_states': self.next_states[idx],\n",
    "            'next_states_flat': self.next_states[idx].reshape(batch_size, self.actions[idx].size),\n",
    "            'dones': self.dones[idx]\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MATD3TwinCriticNetwork(nn.Module):\n",
    "    def __init__(self, all_agents_state_size: int, all_agents_action_size: int, hidden_dims: List[int]=(256, 256), activation_fn = F.relu, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
    "        \"\"\"\n",
    "        Centralized action-value function approximator that takes as input the actions of all agents and the states of all agents, and outputs the Q-value for each agent.\n",
    "        :param all_agents_state_size: Dimension of the states of all agents concatenated.\n",
    "        :param all_agents_action_size: Dimension of the actions of all agents concatenated.\n",
    "        :param hidden_dims: List of hidden dimensions for the fully connected layers.\n",
    "        :param activation_fn: Activation function for the fully connected layers.\n",
    "        :param device: Device on which to run the neural network.\n",
    "        \"\"\"\n",
    "        super(MATD3TwinCriticNetwork, self).__init__()\n",
    "        self.device = device\n",
    "        self.all_agents_state_size = all_agents_state_size\n",
    "        self.all_agents_action_size = all_agents_action_size\n",
    "        self.activation_fn = activation_fn\n",
    "\n",
    "        input_dim = self.all_agents_state_size + self.all_agents_action_size\n",
    "        output_dim = 1\n",
    "\n",
    "        self.layers_a = nn.ModuleList()\n",
    "        self.layers_b = nn.ModuleList()\n",
    "        for i, dim in enumerate(hidden_dims):\n",
    "            if i == 0:\n",
    "                self.layers_a.append(nn.Linear(input_dim, dim))\n",
    "                self.layers_b.append(nn.Linear(input_dim, dim))\n",
    "            else:\n",
    "                self.layers_a.append(nn.Linear(hidden_dims[i-1], dim))\n",
    "                self.layers_b.append(nn.Linear(hidden_dims[i-1], dim))\n",
    "            self.layers_a.append(activation_fn)\n",
    "            self.layers_b.append(activation_fn)\n",
    "        self.layers_a.append(nn.Linear(hidden_dims[-1], output_dim))\n",
    "        self.layers_b.append(nn.Linear(hidden_dims[-1], output_dim))\n",
    "        self.layers_a.append(nn.Tanh())\n",
    "        self.layers_b.append(nn.Tanh())\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, states, actions):\n",
    "        \"\"\"\n",
    "        Takes as input the states and actions of all agents, and outputs the Q-value for each agent.\n",
    "        :param states: States of all agents concatenated.\n",
    "        :param actions: Actions of all agents concatenated.\n",
    "        \"\"\"\n",
    "        x = torch.cat([states, actions], dim=1)\n",
    "        for layer_a, layer_b in zip(self.layers_a, self.layers_b):\n",
    "            xa = layer_a(x)\n",
    "            xb = layer_b(x)\n",
    "        return xa, xb\n",
    "\n",
    "    def Qa(self, states, actions):\n",
    "        \"\"\"\n",
    "        Takes as input the states and actions of all agents, and outputs the Qa-value for each agent.\n",
    "        :param states: States of all agents concatenated.\n",
    "        :param actions: Actions of all agents concatenated.\n",
    "        \"\"\"\n",
    "        x = torch.cat([states, actions], dim=1)\n",
    "        for layer_a in self.layers_a:\n",
    "            xa = layer_a(x)\n",
    "        return xa\n",
    "\n",
    "    def load(self, states, actions, rewards, next_states, dones):"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MATD3ActorNetwork(nn.Module):\n",
    "    def __init__(self, state_size: int, action_size: int, hidden_dims=(256,256), activation_fn = F.relu, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
    "        \"\"\"\n",
    "        Policy that takes as input the state of an agent and outputs the actions for that agent.\n",
    "        :param state_size: Dimension of the state of a single agent.\n",
    "        :param action_size: Dimension of the actions of a single agent.\n",
    "        :param hidden_dims: List of hidden dimensions for the fully connected layers.\n",
    "        :param activation_fn: Activation function for the fully connected layers.\n",
    "        :param device: Device on which to run the neural network.\n",
    "        \"\"\"\n",
    "        super(MATD3ActorNetwork, self).__init__()\n",
    "        self.device = device\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.activation_fn = activation_fn\n",
    "\n",
    "        input_dim = state_size\n",
    "        output_dim = action_size\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i, dim in enumerate(hidden_dims):\n",
    "            if i == 0:\n",
    "                self.layers.append(nn.Linear(input_dim, dim))\n",
    "            else:\n",
    "                self.layers.append(nn.Linear(hidden_dims[i-1], dim))\n",
    "            self.layers.append(activation_fn)\n",
    "        self.layers.append(nn.Linear(hidden_dims[-1], output_dim))\n",
    "        self.layers.append(nn.Tanh())\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Takes as input the state of a single agent and outputs the action for that agent.\n",
    "        :param state: State of the agent.\n",
    "        \"\"\"\n",
    "        x = state\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "class MATD3Agent():\n",
    "    def __init__(self, state_size, action_size, all_agents_state_size, all_agents_action_size, config):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.all_agents_state_size = all_agents_state_size\n",
    "        self.all_agents_action_size = all_agents_action_size\n",
    "        self.config = config\n",
    "\n",
    "        self.critic_online = MATD3TwinCriticNetwork(self.all_agents_state_size, self.all_agents_action_size, hidden_dims=self.config.critic_hidden_dims, activation_fn=self.config.critic_activation_fn)\n",
    "        self.critic_target = MATD3TwinCriticNetwork(self.all_agents_state_size, self.all_agents_action_size, hidden_dims=self.config.critic_hidden_dims, activation_fn=self.config.critic_activation_fn)\n",
    "        self.update_critic_target(1.0)\n",
    "\n",
    "        self.actor_online = MATD3ActorNetwork(self.state_size, self.action_size, hidden_dims=self.config.actor_hidden_dims, activation_fn=self.config.actor_activation_fn)\n",
    "        self.actor_target = MATD3ActorNetwork(self.state_size, self.action_size, hidden_dims=self.config.actor_hidden_dims, activation_fn=self.config.actor_activation_fn)\n",
    "        self.update_actor_target(1.0)\n",
    "\n",
    "        self.value_optimizer = optim.Adam(self.critic_online.parameters(), lr=self.config.critic_lr)\n",
    "        self.policy_optimizer = optim.Adam(self.actor_online.parameters(), lr=self.config.actor_lr)\n",
    "\n",
    "    def optimize(self, agent_idx: int, experiences, target_next_actions, optimize_policy: bool):\n",
    "        \"\"\"\n",
    "        Optimizes the models of the agent.\n",
    "        :param agent_idx: Index of this agent.\n",
    "        :param experiences: Experience sample from the MultiAgentReplayBuffer of all agents.\n",
    "        :param target_next_actions: Noisy target actions for all agents.\n",
    "        :param optimize_policy: Whether to optimize the policy or not.\n",
    "        \"\"\"\n",
    "        batch_size = len(experiences['dones'])\n",
    "        states = experiences['states']\n",
    "        states_flat = experiences['states_flat']\n",
    "        states_self = experiences['states'][:, agent_idx, :]\n",
    "        actions = experiences['actions']\n",
    "        actions_flat = experiences['actions_flat']\n",
    "        rewards_self = experiences['rewards'][:, agent_idx]\n",
    "        next_states_flat = experiences['next_states_flat']\n",
    "        dones_self = experiences['dones'][:, agent_idx]\n",
    "\n",
    "        noisy_target_next_actions_flat = target_next_actions.reshape(batch_size, -1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            qa_target, qb_target = self.critic_target(next_states_flat, noisy_target_next_actions_flat)\n",
    "            target_q = rewards_self + (1 - dones_self) * self.config.gamma * torch.min(qa_target, qb_target)\n",
    "\n",
    "        qa, qb = self.critic_online(states_flat, actions_flat)\n",
    "        value_loss = F.mse_loss(qa, target_q) + F.mse_loss(qb, target_q)\n",
    "        self.value_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        nn.utils.clip_grad_norm(self.critic_online.parameters(), self.config.critic_gradient_clip_value)\n",
    "        self.value_optimizer.step()\n",
    "\n",
    "        if optimize_policy:\n",
    "            greedy_action = self.actor_online(states_self)\n",
    "            actions_copy = np.copy(actions)\n",
    "            actions_copy[:, agent_idx] = greedy_action.detach().cpu().numpy()\n",
    "            q_value = self.critic_online.Qa(states, actions_copy.reshape(batch_size, -1))\n",
    "            policy_loss = -q_value.mean()\n",
    "            self.policy_optimizer.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.actor_online.parameters(), self.config.actor_gradient_clip_value)\n",
    "            self.policy_optimizer.step()\n",
    "\n",
    "    def select_online_action(self, state, std_dev: float, clip_range: float = 1.0):\n",
    "        \"\"\"\n",
    "        Gets the action for exploration from the online actor, taking as input the state of the agent and outputting the greedy action with Gaussian noise added.\n",
    "        :param state: State of the agent. Can be a single state or a batch of states.\n",
    "        :param std_dev: Standard deviation of the distribution.\n",
    "        :param clip_range: Range of the clipped Gaussian noise.\n",
    "        \"\"\"\n",
    "        return self._select_action(self.actor_online, state, std_dev, clip_range)\n",
    "\n",
    "    def select_target_action(self, state, std_dev: float, clip_range: float):\n",
    "        \"\"\"\n",
    "        Gets the action from the target actor, taking as input the state of the agent and outputting the greedy action with Gaussian noise added.\n",
    "        :param state: State of the agent. Can be a single state or a batch of states.\n",
    "        :param std_dev: Standard deviation of the distribution.\n",
    "        :param clip_range: Range of the clipped Gaussian noise.\n",
    "        \"\"\"\n",
    "        return self._select_action(self.actor_target, state, std_dev, clip_range)\n",
    "\n",
    "    def _select_action(self, model, state, std_dev: float, clip_range: float):\n",
    "        \"\"\"\n",
    "        Gets the action from the given model, taking as input the state of the agent and outputting the greedy action.\n",
    "        :param model: Model to use for the action selection.\n",
    "        :param state: State of the agent. Can be a single state or a batch of states.\n",
    "        :param std_dev: Standard deviation of the distribution.\n",
    "        :param clip_range: Range of the clipped Gaussian noise.\n",
    "        \"\"\"\n",
    "        greedy_action = model(state).cpu().detach().numpy()\n",
    "        noise = np.random.normal(0.0, scale=std_dev, size=self.action_size)\n",
    "        noisy_action = np.clip(greedy_action + noise, -abs(clip_range), abs(clip_range))\n",
    "        return noisy_action\n",
    "\n",
    "    def update_critic_target(self, tau: float):\n",
    "        self._update_target(self.critic_target, self.critic_online, tau)\n",
    "\n",
    "    def update_actor_target(self, tau: float):\n",
    "        self._update_target(self.actor_target, self.actor_online, tau)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _update_target(target, online, tau: float):\n",
    "        for target_param, param in zip(target.parameters(), online.parameters()):\n",
    "            target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MATD3Trainer():\n",
    "    def __init__(self, config, env):\n",
    "        self.config = config\n",
    "        torch.manual_seed(self.config.seed); np.random.seed(self.config.seed); random.seed(self.config.seed)\n",
    "\n",
    "        self.env = env\n",
    "        self.brain_name = self.env.brain_names[0]\n",
    "        self.brain = self.env.brains[self.brain_name]\n",
    "        env_info = self.env.reset(train_mode=True)[self.brain_name]\n",
    "        self.num_agents = len(env_info.agents)\n",
    "\n",
    "        self.action_size_per_agent = self.brain.vector_action_space_size\n",
    "        self.all_agents_state_size = env_info.vector_observations.size\n",
    "        self.all_agents_action_size = self.action_size_per_agent * self.num_agents\n",
    "\n",
    "        self.replay_buffer = MultiAgentReplayBuffer(self.config.max_buffer_size, self.num_agents, env_info.vector_observations.shape, self.action_size_per_agent)\n",
    "\n",
    "        self.agents = [MATD3Agent(env_info.vector_observations[agent_idx].size, action_size, self.all_agents_state_size, self.all_agents_action_size, self.config) for agent_idx in range(self.num_agents)]\n",
    "\n",
    "        self.total_timesteps = 0\n",
    "        self.total_episodes = 0\n",
    "        self.episode_rewards = []\n",
    "        self.episode_timesteps = []\n",
    "        self.episode_num = 0\n",
    "\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        for episode in range(1, self.config.max_episodes + 1):\n",
    "            episode_start = time.time()\n",
    "            env_info = self.env.reset(train_mode=True)[self.brain_name]\n",
    "            for step in count():\n",
    "                states = env_info.vector_observations\n",
    "                actions = [agent.select_online_action(states[agent_idx], self.config.exp_std_dev, self.config.exp_clip_range) for agent_idx, agent in enumerate(self.agents)]\n",
    "                actions = np.array(actions)\n",
    "                env_info = self.env.step(actions)[self.brain_name]\n",
    "                next_states = env_info.vector_observations\n",
    "                rewards = env_info.rewards\n",
    "                dones = env_info.local_done\n",
    "                self.replay_buffer.add(states, actions, rewards, next_states, dones)\n",
    "\n",
    "                if len(self.replay_buffer) >= self.config.min_buffer_size:\n",
    "                    experiences = self.replay_buffer.sample(self.config.batch_size)\n",
    "                    noisy_target_next_actions = np.array([agent.select_target_action(experiences['next_states'][:, agent_idx, :], self.config.tps_std_dev, self.config.tps_clip_range) for agent_idx, agent in enumerate(self.agents)])\n",
    "\n",
    "                    optimize_policy = True if step % self.config.optimize_policy_every_steps == 0 else False\n",
    "                    for agent_idx, agent in enumerate(self.agents):\n",
    "                        agent.optimize(agent_idx, experiences, noisy_target_next_actions, optimize_policy)\n",
    "\n",
    "                if step % self.config.update_critic_target_every_steps == 0:\n",
    "                    for agent in self.agents:\n",
    "                        agent.update_critic_target(self.config.tau)\n",
    "\n",
    "                if step % self.config.update_actor_target_every_steps == 0:\n",
    "                    for agent in self.agents:\n",
    "                        agent.update_actor_target(self.config.tau)\n",
    "\n",
    "                if np.any(dones):\n",
    "                    break\n",
    "\n",
    "            episode_end = time.time()\n",
    "            episode_duration = episode_end - episode_start"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [
    {
     "data": {
      "text/plain": "(5, 2, 24)"
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = env_info.vector_observations\n",
    "a = np.full((5, *s.shape), np.nan)\n",
    "for i in range(5):\n",
    "    a[i] = s + i\n",
    "a.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [
    {
     "data": {
      "text/plain": "(5, 48)"
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [
    {
     "data": {
      "text/plain": "(3, 1, 4)"
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[[2,2,2,3]],[[2,2,2,3]],[[2,2,2,3]]]).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}